{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "import medmnist\n",
    "from medmnist import INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fractal_learning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_185185/2853654397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfractal_learning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfractals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mifs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiamondsquare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# データセットを格納するためのリスト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fractal_learning'"
     ]
    }
   ],
   "source": [
    "from fractal_learning.fractals import ifs, diamondsquare\n",
    "import numpy as np\n",
    "\n",
    "# データセットを格納するためのリスト\n",
    "datasets = []\n",
    "\n",
    "# 50回の実行\n",
    "for _ in range(15000):\n",
    "    # フラクタルデータの生成\n",
    "    system = ifs.sample_system()\n",
    "    points = ifs.iterate(system, 100000)\n",
    "\n",
    "    # 画像の生成\n",
    "    binary_image = ifs.render(points, binary=True)\n",
    "    gray_image = ifs.render(points, binary=False)\n",
    "    color_image = ifs.colorize(gray_image)\n",
    "\n",
    "    background = diamondsquare.colorized_ds()\n",
    "\n",
    "    composite = background.copy()\n",
    "    composite[gray_image.nonzero()] = color_image[gray_image.nonzero()]\n",
    "\n",
    "    # 生成された画像をデータセットに追加\n",
    "    datasets.append({\n",
    "        'gray_image': gray_image,\n",
    "    })\n",
    "\n",
    "# datasetsをnumpy配列に変換\n",
    "gray_images = np.array([data['gray_image'] for data in datasets])\n",
    "\n",
    "# それぞれのデータセットの形状を確認\n",
    "print(\"Gray Images Shape:\", gray_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8562/2042107558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# データセット内の'gray_image'を変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtransformed_gray_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray_image\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgray_image\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gray_image'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# リストをPyTorchのテンソルに変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not iterable"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# あらかじめ定義された変換を取得\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(28, 28), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "# データセット内の'gray_image'を変換\n",
    "transformed_gray_images = [transform(gray_image) for gray_image in [data['gray_image'] for data in datasets]]\n",
    "\n",
    "# リストをPyTorchのテンソルに変換\n",
    "transformed_gray_images_tensor = torch.stack(transformed_gray_images)\n",
    "\n",
    "# DataLoaderに変換したテンソルを使う\n",
    "dataset = TensorDataset(transformed_gray_images_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================前までのコード========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_flattened, num_images=1, size=(1, 28, 28)):\n",
    "  image = image_flattened.detach().cpu().view(-1, *size) # 画像のサイズ1x28x28に戻す\n",
    "  image_grid = make_grid(image[:num_images], nrow=2) # 画像を並べる\n",
    "  plt.imshow(image_grid.permute(1, 2, 0).squeeze()) # 画像の表示\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(n_samples, z_dim, device='cuda'):\n",
    "  return torch.randn(n_samples, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 256\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "num_of_epochs = 1\n",
    "device = 'cuda'\n",
    "n_class = 1\n",
    "dataset = \"MNIST\" #FashionMNIST, Fractal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============データセット加工＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"MNIST\":\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize(size=(28, 28), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "                                transforms.Normalize((0.5, ), (0.5, ))])\n",
    " \n",
    "    dataloader = DataLoader(\n",
    "        MNIST('.', download=True, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "elif dataset == \"FashionMNIST\":\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Resize(size=(28, 28), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "                                transforms.Normalize((0.5, ), (0.5, ))])\n",
    " \n",
    "    dataloader = DataLoader(\n",
    "        FashionMNIST('.', download=True, transform=transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "elif dataset == \"Fractal\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize(size=(28, 28), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),  # Convert to PyTorch tensor\n",
    "        transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ])\n",
    "    load_path = os.path.join(\"/workspace/Data/Fractal/\", \"Frac60\")\n",
    "    data = datasets.ImageFolder(load_path, transform=transform)\n",
    "    # DataLoaderの作成\n",
    "    dataloader = DataLoader(data, batch_size=batch_size) \n",
    "\n",
    "elif dataset == \"MedMNIST\":\n",
    "    info = INFO['tissuemnist']\n",
    "\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Resize(size=(28, 28), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "        transforms.Normalize((0.5, ), (0.5, ))\n",
    "        ])\n",
    "    # load the data\n",
    "    train_dataset = DataClass(split='train', download=True, transform=transform)\n",
    "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "else:\n",
    "    raise ValueError(\"エラー\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderから20個のデータを取得\n",
    "num_samples_to_extract = list(range(1,n_class*10))\n",
    "data_extractor = iter(dataloader)\n",
    "new_data = []\n",
    "\n",
    "for _ in (num_samples_to_extract):\n",
    "    try:\n",
    "        sample = next(data_extractor)\n",
    "        new_data.append(sample)\n",
    "    except StopIteration:\n",
    "        # もしデータが尽きたら、この処理でループを終了\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for data in(new_data):\n",
    "    counter += 1\n",
    "    if counter == 21:#37192235\n",
    "        plt.imshow(data[0][0].squeeze(), cmap='gray') # 画像の表示\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ウェイトの初期化\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = nn.BCELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(nn.Module):\n",
    "  def __init__(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "    super(DiscriminatorBlock, self).__init__()\n",
    "    if not final_layer:\n",
    "      self.discriminator_block = nn.Sequential(nn.Conv2d(input_channels, output_channels,\n",
    "                                                        kernel_size, stride),\n",
    "                                              nn.BatchNorm2d(output_channels),\n",
    "                                              nn.LeakyReLU(negative_slope=0.2,\n",
    "                                                           inplace=True))\n",
    "    else:\n",
    "      self.discriminator_block = nn.Sequential(nn.Conv2d(input_channels, output_channels,\n",
    "                                                         kernel_size, stride))\n",
    "  def forward(self, x):\n",
    "    return self.discriminator_block(x) \n",
    "  \n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, image_channels, hidden_channels):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.discriminator = nn.Sequential(DiscriminatorBlock(image_channels, hidden_channels),\n",
    "                                       DiscriminatorBlock(hidden_channels, hidden_channels * 2),\n",
    "                                       DiscriminatorBlock(hidden_channels * 2, 1,\n",
    "                                                          final_layer=True))\n",
    "  def forward(self, input_images):\n",
    "    prediction = self.discriminator(input_images)\n",
    "    print(prediction.size())\n",
    "    return prediction.view(len(prediction), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorA(nn.Module):\n",
    "    def __init__(self, nz=10, ngf=64, nc=1, img_size=28, activation=None, final_bn=False):\n",
    "        super(GeneratorA, self).__init__() \n",
    "\n",
    "        self.init_size = img_size//4\n",
    "        self.l1 = nn.Sequential(nn.Linear(nz, ngf*2*self.init_size**2))\n",
    "        self.conv_blocks0 = nn.Sequential(\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "        )\n",
    "        self.conv_blocks1 = nn.Sequential(\n",
    "            nn.Conv2d(ngf*2, ngf*2, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        if final_bn:\n",
    "            self.conv_blocks2 = nn.Sequential(\n",
    "                nn.Conv2d(ngf*2, ngf, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ngf, nc, 3, stride=1, padding=1),\n",
    "                # nn.Tanh(),\n",
    "                nn.BatchNorm2d(nc, affine=False) \n",
    "            )\n",
    "        else:\n",
    "            self.conv_blocks2 = nn.Sequential(\n",
    "                nn.Conv2d(ngf*2, ngf, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(ngf),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(ngf, nc, 3, stride=1, padding=1),\n",
    "                # nn.Tanh(),\n",
    "                # nn.BatchNorm2d(nc, affine=False) \n",
    "            )\n",
    "\n",
    "    def forward(self, z, pre_x=True):\n",
    "        z = self.l1(z.view(z.shape[0],-1))\n",
    "        out = z.view(z.shape[0], -1, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks0(out)\n",
    "        img = nn.functional.interpolate(img,scale_factor=2)\n",
    "        img = self.conv_blocks1(img)\n",
    "        img = nn.functional.interpolate(img,scale_factor=2)\n",
    "        img = self.conv_blocks2(img)\n",
    "\n",
    "        if pre_x :\n",
    "            return img\n",
    "        else:\n",
    "            # img = nn.functional.interpolate(img, scale_factor=2)\n",
    "            return nn.Tanh(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_185185/1526307392.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# インスタンス化\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# generatorA = GeneratorA(nz = z_dim, img_size = 28).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgeneratorA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneratorA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_channles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    924\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "image_channels = 1\n",
    "hidden_channles = 16\n",
    "# インスタンス化\n",
    "# generatorA = GeneratorA(nz = z_dim, img_size = 28).to(device) \n",
    "generatorA = GeneratorA(nz=z_dim, nc=1, img_size=28, activation=torch.tanh, final_bn=False).to(device)\n",
    "discriminator = Discriminator(image_channels=image_channels, hidden_channels=hidden_channles).to(device)\n",
    " \n",
    "# オプティマイザ\n",
    "gen_optA = torch.optim.Adam(generatorA.parameters(), lr=learning_rate, betas=(beta_1, beta_2))# , betas=(beta_1, beta_2), weight_decay=1e-5)\n",
    "disc_opt = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(beta_1, beta_2), weight_decay=1e-5)# , betas=(beta_1, beta_2)\n",
    "\n",
    "discriminator = discriminator.apply(weights_init)\n",
    "# generatorA = generatorA.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_of_epochs):\n",
    "  mean_generator_loss = 0\n",
    "  mean_discriminator_loss = 0\n",
    "  for real_images, _ in tqdm(dataloader):\n",
    "    real_images = real_images.to(device)\n",
    "\n",
    "    for i in range(5):\n",
    "      # discriminator\n",
    "      disc_opt.zero_grad() # 勾配の初期化\n",
    "      # 偽画像\n",
    "\n",
    "      noise = get_noise(len(real_images), z_dim, device=device) # ノイズの生成\n",
    "      fake_images = generatorA(noise, pre_x=True) # 偽画像を生成\n",
    "      disc_fake_prediction = discriminator(fake_images.detach()) # Discriminatorで予測\n",
    "      correct_labels = torch.zeros_like(disc_fake_prediction) # 偽画像の正解ラベルは\n",
    "      disc_fake_loss = criterion(disc_fake_prediction, correct_labels) # 偽画像に対する損失を計算\n",
    "\n",
    "  \n",
    "      # 本物の画像\n",
    "      disc_real_prediction = discriminator(real_images) # Discriminatorで予測\n",
    "      correct_labels = torch.ones_like(disc_real_prediction) # 本物の画像の正解ラベルは1\n",
    "      disc_real_loss = criterion(disc_real_prediction, correct_labels * 0.9) # 本物の画像に対する損失を計算\n",
    "  \n",
    "      # 最終的な損失\n",
    "      disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "      disc_loss.backward()\n",
    "      disc_opt.step()\n",
    "  \n",
    "      # エポックごとの損失\n",
    "      mean_discriminator_loss += disc_loss / len(real_images)\n",
    "    \n",
    "    for i in range(1):\n",
    "      # generator\n",
    "      gen_optA.zero_grad() # 勾配の初期化\n",
    "      fake_noise = get_noise(len(real_images), z_dim, device=device) # ノイズの生成\n",
    "      fake_images = generatorA(fake_noise, pre_x=True) # 偽画像の生成\n",
    "      disc_fake_prediction = discriminator(fake_images) # Discriminatorで予測\n",
    "      correct_labels = torch.ones_like(disc_fake_prediction) # 本物の正解ラベルは1\n",
    "      gen_loss = criterion(disc_fake_prediction, correct_labels * 0.9) # 損失を計算\n",
    "      gen_loss.backward()\n",
    "      gen_optA.step()\n",
    "      # エポックごとの損失\n",
    "      mean_generator_loss += gen_loss / len(real_images)\n",
    "  # print(f'Generator loss: {mean_generator_loss}')\n",
    "  # print(f'Discriminator loss: {mean_discriminator_loss}')\n",
    "   \n",
    "  # 生成される画像を表示\n",
    "  # noise = get_noise(len(real_images), z_dim, device=device)\n",
    "  # print(generatorA(noise).size())\n",
    "  # show_tensor_images(generatorA(noise))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 生成される画像を表示\n",
    "noise = get_noise(128, z_dim, device=device)\n",
    "print(generatorA(noise).size())\n",
    "show_tensor_images(generatorA(noise))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=============モデル保存========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"OCTMNIST\"\n",
    "midpath = os.path.join('/workspace/network_Gan/', dataset)\n",
    "savepath = os.path.join(midpath, 'net_param.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generatorA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1739524/2195105860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneratorA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msavepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generatorA' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(generatorA.state_dict(),savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================コードの確認＝＝＝＝＝＝＝＝＝＝＝＝＝＝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MedMNIST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "midpath = os.path.join('/workspace/network_Gan/', dataset)\n",
    "savepath = os.path.join(midpath, 'net_param.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generatorA = GeneratorA(nz = z_dim, img_size = 28).to(device) \n",
    "generatorA.load_state_dict(torch.load(savepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_flattened, num_images=2, size=(1, 28, 28)):\n",
    "  image = image_flattened.detach().cpu().view(-1, *size) # 画像のサイズ1x28x28に戻す\n",
    "  image_grid = make_grid(image[:num_images], nrow=2) # 画像を並べる\n",
    "  plt.imshow(image_grid.permute(1, 2, 0).squeeze()) # 画像の表示\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 生成される画像を表示\n",
    "noise = get_noise(128, z_dim, device=device)\n",
    "print(generatorA(noise).size())\n",
    "show_tensor_images(generatorA(noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
